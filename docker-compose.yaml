services:
  postgres:
    image: postgres:13.8-bullseye
    hostname: postgres
    container_name: postgres
    restart: always
    environment:
      POSTGRES_HOST_AUTH_METHOD: trust
      PGDATA: /var/lib/postgresql/data
    ports:
      - "5432:5432"
    volumes:
      - ./tmp/postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 5s
      retries: 5
  redis: 
    image: redis:6.0.16-bullseye
    hostname: redis
    container_name: redis
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - ./tmp/redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
  mongo:
    image: mongo:6.0.2-focal
    hostname: mongo
    container_name: mongo
    restart: always
    ports:
      - "27017:27017"
    volumes:
      - ./tmp/mongo-data:/data/db

  # pipeline-api:
  #   build:
  #     context: ./pipeline_api
  #     dockerfile: Dockerfile
  #   image: nmngo248/pipeline-api:latest
  #   container_name: pipeline-api
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     - MONGO_HOST=mongo
  #     - MONGO_PORT=27017
  #     - POSTGRES_HOST=postgres
  #     - POSTGRES_PORT=5432
  #     - POSTGRES_DB_STAGING=deng_staging
  #     - POSTGRES_USER=postgres
  #     - POSTGRES_PASSWORD=
  #     - REDIS_HOST=redis
  #     - REDIS_PORT=6379
  #     - REDIS_DB=0
  #     - PYTHONUNBUFFERED=1
  #     - TZ=Europe/Paris
  #   volumes:
  #     - ./pipeline_api/bus_data:/app/bus_data
  #     - ./pipeline_api/logs:/app/logs
  #   env_file:
  #     - .env_api 
  #   hostname: pipeline-api
  #   depends_on:
  #     - mongo
  #     - postgres
  #     - redis

  postgres-airflow:
    container_name: postgres-airflow
    extends:
      file: airflow-compose.yaml
      service: postgres-airflow
  redis-airflow:
    container_name: redis-airflow
    extends:
      file: airflow-compose.yaml
      service: redis-airflow
  airflow-webserver:
    container_name: airflow-webserver
    extends:
      file: airflow-compose.yaml
      service: airflow-webserver
  airflow-scheduler:
    container_name: airflow-scheduler
    extends:
      file: airflow-compose.yaml
      service: airflow-scheduler
  airflow-worker:
    container_name: airflow-worker
    extends:
      file: airflow-compose.yaml
      service: airflow-worker  
  airflow-init:
    container_name: airflow-init
    profiles: ["init"]
    extends:
      file: airflow-compose.yaml
      service: airflow-init
  flower:
    container_name: airflow-flower
    extends:
      file: airflow-compose.yaml
      service: flower

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
    volumes:
      - ./tmp/hadoop-namenode:/hadoop/dfs/name
      - ./hadoop/data:/hadoop-data
    hostname: namenode
    environment:
      CLUSTER_NAME: hadoop

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    volumes:
      - ./tmp/hadoop-datanode:/hadoop/dfs/data
      - ./hadoop/data:/hadoop-data
    hostname: datanode
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CLUSTER_NAME: hadoop

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    ports:
      - "8088:8088"
    hostname: resourcemanager
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864"
      CLUSTER_NAME: hadoop

  spark-master:
    container_name: spark-master
    extends:
      file: spark-compose.yaml
      service: spark-master
  spark-worker:
    container_name: spark-worker
    extends:
      file: spark-compose.yaml
      service: spark-worker
networks:
  default:
    name: meteorif